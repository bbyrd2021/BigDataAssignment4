{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e0477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4141b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 16:20:14.512272: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-24 16:20:14.512300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-24 16:20:14.513397: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-24 16:20:14.519095: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-24 16:20:15.026358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "Logical devices: [LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "Physical devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "True\n",
      "WARNING:tensorflow:From /tmp/ipykernel_794774/2383452234.py:8: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 16:20:15.617001: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-11-24 16:20:15.617025: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: hmoradi-Lambda-Vector\n",
      "2025-11-24 16:20:15.617030: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: hmoradi-Lambda-Vector\n",
      "2025-11-24 16:20:15.617102: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 550.107.2\n",
      "2025-11-24 16:20:15.617117: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 550.107.2\n",
      "2025-11-24 16:20:15.617121: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 550.107.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "print(\"Logical devices:\", tf.config.list_logical_devices())\n",
    "print(\"Physical devices:\", tf.config.list_physical_devices())\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40372ce9",
   "metadata": {},
   "source": [
    "# Tiny ImageNet Classification Assignment\n",
    "\n",
    "This notebook completes the requirements for the Tiny ImageNet assignment.\n",
    "\n",
    "The goals are:\n",
    "\n",
    "1. Data exploration  \n",
    "2. Training a baseline convolutional neural network model  \n",
    "3. Reporting model architecture, training strategy, results, and improvements  \n",
    "\n",
    "The dataset has 200 classes of 64x64 RGB images. GPU acceleration is used to speed up training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c471e",
   "metadata": {},
   "source": [
    "## Imports and GPU Verification\n",
    "\n",
    "This cell imports all required libraries for data loading, preprocessing, training, and evaluation.  \n",
    "It also verifies that TensorFlow can detect the GPU devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97c5ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detected: []\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Verify GPU visibility\n",
    "print(\"GPUs detected:\", tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e5628",
   "metadata": {},
   "source": [
    "## Download Tiny ImageNet with KaggleHub\n",
    "This cell downloads the Tiny ImageNet dataset using KaggleHub and prints the local path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "382c831f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brandon/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/brandon/.cache/kagglehub/datasets/akash2sharma/tiny-imagenet/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download the latest version of the dataset\n",
    "path = kagglehub.dataset_download(\"akash2sharma/tiny-imagenet\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e0e7d",
   "metadata": {},
   "source": [
    "## Inspect Dataset Directory Structure\n",
    "\n",
    "The Tiny ImageNet dataset has been downloaded using KaggleHub.  \n",
    "Before building data loaders or reorganizing folders, it is important to inspect the actual directory structure.  \n",
    "This cell displays the top level of the dataset path so we can confirm the presence of expected folders such as train, val, test, wnids.txt, and words.txt.\n",
    "\n",
    "The folder structure will guide how the dataset loading code is written in later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c032fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/brandon/.cache/kagglehub/datasets/akash2sharma/tiny-imagenet/versions/1\n",
      "DIRS: ['tiny-imagenet-200']\n",
      "FILES: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_ROOT = \"/home/brandon/.cache/kagglehub/datasets/akash2sharma/tiny-imagenet/versions/1\"\n",
    "\n",
    "for root, dirs, files in os.walk(DATA_ROOT):\n",
    "    print(\"ROOT:\", root)\n",
    "    print(\"DIRS:\", dirs)\n",
    "    print(\"FILES:\", files[:10])\n",
    "    print()\n",
    "    break  # show only the top level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033ea22",
   "metadata": {},
   "source": [
    "## Inspect Contents of tiny-imagenet-200\n",
    "\n",
    "The top level of the dataset contains a single directory named tiny-imagenet-200.  \n",
    "This cell inspects the contents of that directory to verify the presence of the expected subfolders and files.  \n",
    "These typically include train, val, test, wnids.txt, and words.txt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7502d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/brandon/.cache/kagglehub/datasets/akash2sharma/tiny-imagenet/versions/1/tiny-imagenet-200\n",
      "DIRS: ['val', 'test', 'train', 'tiny-imagenet-200']\n",
      "FILES: ['words.txt', 'wnids.txt']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inner_root = os.path.join(DATA_ROOT, \"tiny-imagenet-200\")\n",
    "\n",
    "for root, dirs, files in os.walk(inner_root):\n",
    "    print(\"ROOT:\", root)\n",
    "    print(\"DIRS:\", dirs)\n",
    "    print(\"FILES:\", files[:10])\n",
    "    print()\n",
    "    break  # only show one level for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a635ef",
   "metadata": {},
   "source": [
    "## Inspect train, val, and test folders\n",
    "\n",
    "The dataset contains the expected structure with train, val, and test directories.  \n",
    "The train directory should contain 200 class subfolders.  \n",
    "The val directory contains images and a file that maps image names to class labels.  \n",
    "Before loading the dataset, this cell inspects the internal structure of these folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c059c576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train directory sample:\n",
      "['n03089624', 'n02815834', 'n01629819', 'n03617480', 'n02988304', 'n03599486', 'n04285008', 'n03854065', 'n03201208', 'n01768244']\n",
      "\n",
      "Val directory sample:\n",
      "['n03089624', 'n02815834', 'n01629819', 'n03617480', 'n02988304', 'n03599486', 'n04285008', 'n03854065', 'n03201208', 'n01768244']\n",
      "\n",
      "Test directory sample:\n",
      "['images']\n",
      "\n",
      "Nested tiny-imagenet-200 directory exists. Contents:\n",
      "['words.txt', 'val', 'wnids.txt', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "# Inspect train directory\n",
    "train_dir = os.path.join(inner_root, \"train\")\n",
    "print(\"Train directory sample:\")\n",
    "print(os.listdir(train_dir)[:10])\n",
    "\n",
    "# Inspect val directory\n",
    "val_dir = os.path.join(inner_root, \"val\")\n",
    "print(\"\\nVal directory sample:\")\n",
    "print(os.listdir(val_dir)[:10])\n",
    "\n",
    "# Inspect test directory\n",
    "test_dir = os.path.join(inner_root, \"test\")\n",
    "print(\"\\nTest directory sample:\")\n",
    "print(os.listdir(test_dir)[:10])\n",
    "\n",
    "# Inspect the unexpected nested folder, if needed\n",
    "nested_dir = os.path.join(inner_root, \"tiny-imagenet-200\")\n",
    "if os.path.exists(nested_dir):\n",
    "    print(\"\\nNested tiny-imagenet-200 directory exists. Contents:\")\n",
    "    print(os.listdir(nested_dir)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375d461",
   "metadata": {},
   "source": [
    "## Set Dataset Root Directory\n",
    "\n",
    "The path to the Tiny ImageNet dataset is defined here.  \n",
    "This path was obtained using KaggleHub.  \n",
    "All later data loading operations will reference this directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0483637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root: /home/brandon/.cache/kagglehub/datasets/akash2sharma/tiny-imagenet/versions/1/tiny-imagenet-200\n",
      "Contents: ['words.txt', 'val', 'wnids.txt', 'test', 'train', 'tiny-imagenet-200']\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"/home/brandon/.cache/kagglehub/datasets/akash2sharma/tiny-imagenet/versions/1/tiny-imagenet-200\"\n",
    "\n",
    "print(\"Dataset root:\", DATA_ROOT)\n",
    "print(\"Contents:\", os.listdir(DATA_ROOT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab0886",
   "metadata": {},
   "source": [
    "## Fix Validation Folder Structure\n",
    "\n",
    "The Tiny ImageNet validation images are stored in a single folder named images, with labels in val_annotations.txt.  \n",
    "Keras requires validation images to be organized into class subfolders.  \n",
    "This cell reorganizes validation images into class-specific folders.  \n",
    "This operation only needs to run once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44aafa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation folder already organized.\n"
     ]
    }
   ],
   "source": [
    "val_dir = os.path.join(DATA_ROOT, \"val\")\n",
    "val_images_dir = os.path.join(val_dir, \"images\")\n",
    "annotations_file = os.path.join(val_dir, \"val_annotations.txt\")\n",
    "\n",
    "if os.path.exists(val_images_dir):\n",
    "    print(\"Reorganizing validation images...\")\n",
    "\n",
    "    # Read the mapping from image to class\n",
    "    with open(annotations_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            img_name, class_id = parts[0], parts[1]\n",
    "\n",
    "            class_dir = os.path.join(val_dir, class_id)\n",
    "            if not os.path.exists(class_dir):\n",
    "                os.makedirs(class_dir)\n",
    "\n",
    "            src = os.path.join(val_images_dir, img_name)\n",
    "            dst = os.path.join(class_dir, img_name)\n",
    "\n",
    "            if os.path.exists(src):\n",
    "                shutil.move(src, dst)\n",
    "\n",
    "    shutil.rmtree(val_images_dir)\n",
    "    print(\"Validation folder reorganized successfully.\")\n",
    "\n",
    "else:\n",
    "    print(\"Validation folder already organized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81c9cb",
   "metadata": {},
   "source": [
    "### Verify Train Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa8d5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: 200\n",
      "['n03089624', 'n02815834', 'n01629819', 'n03617480', 'n02988304', 'n03599486', 'n04285008', 'n03854065', 'n03201208', 'n01768244']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Train classes:\", len(os.listdir(os.path.join(DATA_ROOT, \"train\"))))\n",
    "print(os.listdir(os.path.join(DATA_ROOT, \"train\"))[:10])\n",
    "\n",
    "# print(\"\\nVal classes:\", len(os.listdir(os.path.join(DATA_ROOT, \"val\"))))\n",
    "# print(os.listdir(os.path.join(DATA_ROOT, \"val\"))[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bbff2",
   "metadata": {},
   "source": [
    "## Fix Validation Folder Structure\n",
    "\n",
    "Tiny ImageNet stores validation images in a single folder named images and provides labels in val_annotations.txt.  \n",
    "This cell reorganizes validation images into class folders.  \n",
    "This operation must be performed only once and must delete val_annotations.txt afterward to avoid mislabeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ecd9e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation classes: 200\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "val_root = os.path.join(DATA_ROOT, \"val\")\n",
    "val_images = os.path.join(val_root, \"images\")\n",
    "annotations = os.path.join(val_root, \"val_annotations.txt\")\n",
    "\n",
    "if os.path.exists(val_images):\n",
    "    print(\"Reorganizing validation directory.\")\n",
    "\n",
    "    # Read mappings\n",
    "    mapping = {}\n",
    "    with open(annotations, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            img, cls = parts[0], parts[1]\n",
    "            mapping[img] = cls\n",
    "\n",
    "    # Create class folders and move images\n",
    "    for img, cls in mapping.items():\n",
    "        class_dir = os.path.join(val_root, cls)\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "\n",
    "        src = os.path.join(val_images, img)\n",
    "        dst = os.path.join(class_dir, img)\n",
    "\n",
    "        if os.path.exists(src):\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "    # Remove the images folder\n",
    "    shutil.rmtree(val_images)\n",
    "\n",
    "# IMPORTANT: Delete the annotations file so Keras does not treat it as a class folder\n",
    "if os.path.exists(annotations):\n",
    "    print(\"Removing annotation file to avoid Keras mislabeling:\", annotations)\n",
    "    os.remove(annotations)\n",
    "\n",
    "print(\"Validation classes:\", len(os.listdir(val_root)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ad4d3",
   "metadata": {},
   "source": [
    "## Verify Final Class Count\n",
    "Both train and val must have exactly 200 folders.  \n",
    "Any deviation indicates a labeling mismatch that will break training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbd60ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class count: 200\n",
      "Val class count: 200\n"
     ]
    }
   ],
   "source": [
    "train_classes = os.listdir(os.path.join(DATA_ROOT, \"train\"))\n",
    "val_classes = os.listdir(os.path.join(DATA_ROOT, \"val\"))\n",
    "\n",
    "print(\"Train class count:\", len(train_classes))\n",
    "print(\"Val class count:\", len(val_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63126a",
   "metadata": {},
   "source": [
    "## Create Training and Validation Generators\n",
    "\n",
    "This cell creates the Keras ImageDataGenerator objects that will feed images into the model.  \n",
    "The training generator includes simple augmentation such as horizontal flip and rotation.  \n",
    "Both generators rescale pixel values to the range [0, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "662a2a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n",
      "Found 10000 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMAGE_SIZE = (64, 64)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=10\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(DATA_ROOT, \"train\"),\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    os.path.join(DATA_ROOT, \"val\"),\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d28b20a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN KEYS: ['n01443537', 'n01629819', 'n01641577', 'n01644900', 'n01698640', 'n01742172', 'n01768244', 'n01770393', 'n01774384', 'n01774750', 'n01784675', 'n01855672', 'n01882714', 'n01910747', 'n01917289', 'n01944390', 'n01945685', 'n01950731', 'n01983481', 'n01984695', 'n02002724', 'n02056570', 'n02058221', 'n02074367', 'n02085620', 'n02094433', 'n02099601', 'n02099712', 'n02106662', 'n02113799', 'n02123045', 'n02123394', 'n02124075', 'n02125311', 'n02129165', 'n02132136', 'n02165456', 'n02190166', 'n02206856', 'n02226429', 'n02231487', 'n02233338', 'n02236044', 'n02268443', 'n02279972', 'n02281406', 'n02321529', 'n02364673', 'n02395406', 'n02403003', 'n02410509', 'n02415577', 'n02423022', 'n02437312', 'n02480495', 'n02481823', 'n02486410', 'n02504458', 'n02509815', 'n02666196', 'n02669723', 'n02699494', 'n02730930', 'n02769748', 'n02788148', 'n02791270', 'n02793495', 'n02795169', 'n02802426', 'n02808440', 'n02814533', 'n02814860', 'n02815834', 'n02823428', 'n02837789', 'n02841315', 'n02843684', 'n02883205', 'n02892201', 'n02906734', 'n02909870', 'n02917067', 'n02927161', 'n02948072', 'n02950826', 'n02963159', 'n02977058', 'n02988304', 'n02999410', 'n03014705', 'n03026506', 'n03042490', 'n03085013', 'n03089624', 'n03100240', 'n03126707', 'n03160309', 'n03179701', 'n03201208', 'n03250847', 'n03255030', 'n03355925', 'n03388043', 'n03393912', 'n03400231', 'n03404251', 'n03424325', 'n03444034', 'n03447447', 'n03544143', 'n03584254', 'n03599486', 'n03617480', 'n03637318', 'n03649909', 'n03662601', 'n03670208', 'n03706229', 'n03733131', 'n03763968', 'n03770439', 'n03796401', 'n03804744', 'n03814639', 'n03837869', 'n03838899', 'n03854065', 'n03891332', 'n03902125', 'n03930313', 'n03937543', 'n03970156', 'n03976657', 'n03977966', 'n03980874', 'n03983396', 'n03992509', 'n04008634', 'n04023962', 'n04067472', 'n04070727', 'n04074963', 'n04099969', 'n04118538', 'n04133789', 'n04146614', 'n04149813', 'n04179913', 'n04251144', 'n04254777', 'n04259630', 'n04265275', 'n04275548', 'n04285008', 'n04311004', 'n04328186', 'n04356056', 'n04366367', 'n04371430', 'n04376876', 'n04398044', 'n04399382', 'n04417672', 'n04456115', 'n04465501', 'n04486054', 'n04487081', 'n04501370', 'n04507155', 'n04532106', 'n04532670', 'n04540053', 'n04560804', 'n04562935', 'n04596742', 'n04597913', 'n06596364', 'n07579787', 'n07583066', 'n07614500', 'n07615774', 'n07695742', 'n07711569', 'n07715103', 'n07720875', 'n07734744', 'n07747607', 'n07749582', 'n07753592', 'n07768694', 'n07871810', 'n07873807', 'n07875152', 'n07920052', 'n09193705', 'n09246464', 'n09256479', 'n09332890', 'n09428293', 'n12267677']\n",
      "VAL KEYS: ['n01443537', 'n01629819', 'n01641577', 'n01644900', 'n01698640', 'n01742172', 'n01768244', 'n01770393', 'n01774384', 'n01774750', 'n01784675', 'n01855672', 'n01882714', 'n01910747', 'n01917289', 'n01944390', 'n01945685', 'n01950731', 'n01983481', 'n01984695', 'n02002724', 'n02056570', 'n02058221', 'n02074367', 'n02085620', 'n02094433', 'n02099601', 'n02099712', 'n02106662', 'n02113799', 'n02123045', 'n02123394', 'n02124075', 'n02125311', 'n02129165', 'n02132136', 'n02165456', 'n02190166', 'n02206856', 'n02226429', 'n02231487', 'n02233338', 'n02236044', 'n02268443', 'n02279972', 'n02281406', 'n02321529', 'n02364673', 'n02395406', 'n02403003', 'n02410509', 'n02415577', 'n02423022', 'n02437312', 'n02480495', 'n02481823', 'n02486410', 'n02504458', 'n02509815', 'n02666196', 'n02669723', 'n02699494', 'n02730930', 'n02769748', 'n02788148', 'n02791270', 'n02793495', 'n02795169', 'n02802426', 'n02808440', 'n02814533', 'n02814860', 'n02815834', 'n02823428', 'n02837789', 'n02841315', 'n02843684', 'n02883205', 'n02892201', 'n02906734', 'n02909870', 'n02917067', 'n02927161', 'n02948072', 'n02950826', 'n02963159', 'n02977058', 'n02988304', 'n02999410', 'n03014705', 'n03026506', 'n03042490', 'n03085013', 'n03089624', 'n03100240', 'n03126707', 'n03160309', 'n03179701', 'n03201208', 'n03250847', 'n03255030', 'n03355925', 'n03388043', 'n03393912', 'n03400231', 'n03404251', 'n03424325', 'n03444034', 'n03447447', 'n03544143', 'n03584254', 'n03599486', 'n03617480', 'n03637318', 'n03649909', 'n03662601', 'n03670208', 'n03706229', 'n03733131', 'n03763968', 'n03770439', 'n03796401', 'n03804744', 'n03814639', 'n03837869', 'n03838899', 'n03854065', 'n03891332', 'n03902125', 'n03930313', 'n03937543', 'n03970156', 'n03976657', 'n03977966', 'n03980874', 'n03983396', 'n03992509', 'n04008634', 'n04023962', 'n04067472', 'n04070727', 'n04074963', 'n04099969', 'n04118538', 'n04133789', 'n04146614', 'n04149813', 'n04179913', 'n04251144', 'n04254777', 'n04259630', 'n04265275', 'n04275548', 'n04285008', 'n04311004', 'n04328186', 'n04356056', 'n04366367', 'n04371430', 'n04376876', 'n04398044', 'n04399382', 'n04417672', 'n04456115', 'n04465501', 'n04486054', 'n04487081', 'n04501370', 'n04507155', 'n04532106', 'n04532670', 'n04540053', 'n04560804', 'n04562935', 'n04596742', 'n04597913', 'n06596364', 'n07579787', 'n07583066', 'n07614500', 'n07615774', 'n07695742', 'n07711569', 'n07715103', 'n07720875', 'n07734744', 'n07747607', 'n07749582', 'n07753592', 'n07768694', 'n07871810', 'n07873807', 'n07875152', 'n07920052', 'n09193705', 'n09246464', 'n09256479', 'n09332890', 'n09428293', 'n12267677']\n",
      "TRAIN LEN: 200\n",
      "VAL LEN: 200\n",
      "Mapping difference: set()\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN KEYS:\", list(train_generator.class_indices.keys()))\n",
    "print(\"VAL KEYS:\", list(val_generator.class_indices.keys()))\n",
    "\n",
    "print(\"TRAIN LEN:\", len(train_generator.class_indices))\n",
    "print(\"VAL LEN:\", len(val_generator.class_indices))\n",
    "\n",
    "# Check if the mappings differ\n",
    "print(\"Mapping difference:\", set(train_generator.class_indices.keys()) ^ set(val_generator.class_indices.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5506effe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: (64, 64, 64, 3)\n",
      "Batch Y shape: (64, 200)\n",
      "Row sums (should all be 1): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Unique labels: [2, 3, 7, 11, 12, 19, 20, 21, 23, 24, 28, 39, 43, 49, 52, 53, 54, 58, 62, 63, 76, 77, 81, 83, 85, 88, 93, 104, 106, 108, 112, 115, 118, 121, 124, 127, 128, 132, 136, 142, 144, 147, 149, 156, 157, 158, 162, 169, 170, 173, 177, 180, 185, 187, 192, 194, 198, 199]\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(train_generator)\n",
    "\n",
    "print(\"Batch X shape:\", x_batch.shape)\n",
    "print(\"Batch Y shape:\", y_batch.shape)\n",
    "\n",
    "# For classification, Y should be one hot vectors with exactly one 1 per row\n",
    "print(\"Row sums (should all be 1):\", y_batch.sum(axis=1)[:20])\n",
    "\n",
    "# Unique class indices in this batch\n",
    "print(\"Unique labels:\", sorted(list(set(y_batch.argmax(axis=1)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85fb575",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = next(train_generator)\n",
    "\n",
    "print(\"Batch X shape:\", batch_x.shape)\n",
    "print(\"Batch Y shape:\", batch_y.shape)\n",
    "\n",
    "# Show min and max values\n",
    "print(\"Min pixel:\", batch_x.min())\n",
    "print(\"Max pixel:\", batch_x.max())\n",
    "\n",
    "# Show one label\n",
    "print(\"Sample one hot vector:\", batch_y[0])\n",
    "print(\"Label index:\", np.argmax(batch_y[0]))\n",
    "\n",
    "plt.imshow(batch_x[0])\n",
    "plt.title(f\"Label index: {np.argmax(batch_y[0])}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c9a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_generator.directory)\n",
    "print(val_generator.directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e43ace",
   "metadata": {},
   "source": [
    "## Sample Image Visualization\n",
    "\n",
    "This cell displays a grid of sample images drawn from the training generator.  \n",
    "This is part of the data exploration requirement for the assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be122bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(train_generator)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(x_batch[i])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b298f2f0",
   "metadata": {},
   "source": [
    "### Image Quality\n",
    "\n",
    "Image quality looks pretty grainy and low resolution. It makes sense because the image size is so small. (64 x 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9676ede6",
   "metadata": {},
   "source": [
    "## Class Distribution\n",
    "\n",
    "This cell computes the number of training examples for each class.  \n",
    "Tiny ImageNet is balanced by design, but reporting class distribution is required for the assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = {cls: 0 for cls in train_generator.class_indices.keys()}\n",
    "\n",
    "for label_index in train_generator.classes:\n",
    "    class_name = list(train_generator.class_indices.keys())[label_index]\n",
    "    class_counts[class_name] += 1\n",
    "\n",
    "#bar plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(class_counts)), list(class_counts.values()))\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Class Index\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.show()\n",
    "\n",
    "# Display a sample of the dictionary\n",
    "list(class_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b2aff0",
   "metadata": {},
   "source": [
    "### ResNet18 Architecture (from scratch)\n",
    "\n",
    "For Tiny ImageNet we use a ResNet18 style convolutional neural network trained entirely from scratch. ResNet18 is a widely used deep residual architecture that improves optimization by adding shortcut (skip) connections around small stacks of convolutional layers.\n",
    "\n",
    "Key ideas:\n",
    "\n",
    "1. Residual blocks  \n",
    "   - Each block has two 3x3 Conv2D layers with Batch Normalization and ReLU.  \n",
    "   - A skip connection adds the input of the block to its output.  \n",
    "   - If the spatial size or channel count changes, a 1x1 convolution is used on the skip path to match shapes.  \n",
    "   - This helps gradients flow through many layers and avoids vanishing gradient problems.\n",
    "\n",
    "2. Overall structure for 64x64 images  \n",
    "   - Initial 3x3 convolution with 64 filters and BatchNorm + ReLU.  \n",
    "   - Four residual stages with 2 blocks each:  \n",
    "     - Stage 1: 64 channels, stride 1.  \n",
    "     - Stage 2: 128 channels, first block stride 2.  \n",
    "     - Stage 3: 256 channels, first block stride 2.  \n",
    "     - Stage 4: 512 channels, first block stride 2.  \n",
    "   - Global average pooling to reduce each 2D feature map to a single value.  \n",
    "   - Final Dense layer with 200 outputs and softmax for Tiny ImageNet classes.\n",
    "\n",
    "3. Why this is appropriate for Tiny ImageNet  \n",
    "   - Tiny ImageNet has 200 classes with fairly complex appearance variation.  \n",
    "   - Shallow CNNs struggle to learn good features for this dataset.  \n",
    "   - ResNet18 adds depth while keeping optimization stable, which allows the model to learn more expressive hierarchical features.  \n",
    "   - We train with categorical cross entropy loss and report accuracy on the validation set.\n",
    "\n",
    "This model is implemented using `tf.keras` with the functional API, without any pretrained weights. All weights are initialized randomly and learned directly from the Tiny ImageNet data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def residual_block(x, filters, stride=1, use_projection=False, name=None):\n",
    "    \"\"\"\n",
    "    Basic ResNet block with two 3x3 convolutions.\n",
    "\n",
    "    x              - input tensor\n",
    "    filters        - number of output channels\n",
    "    stride         - stride for the first conv (used for downsampling)\n",
    "    use_projection - if True, use 1x1 conv on the skip path to match shape\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "\n",
    "    # First conv\n",
    "    x = layers.Conv2D(filters, (3, 3), strides=stride, padding=\"same\",\n",
    "                      use_bias=False, name=None if name is None else name + \"_conv1\")(x)\n",
    "    x = layers.BatchNormalization(axis=-1, name=None if name is None else name + \"_bn1\")(x)\n",
    "    x = layers.ReLU(name=None if name is None else name + \"_relu1\")(x)\n",
    "\n",
    "    # Second conv\n",
    "    x = layers.Conv2D(filters, (3, 3), strides=1, padding=\"same\",\n",
    "                      use_bias=False, name=None if name is None else name + \"_conv2\")(x)\n",
    "    x = layers.BatchNormalization(axis=-1, name=None if name is None else name + \"_bn2\")(x)\n",
    "\n",
    "    # Projection on shortcut if needed to match shape\n",
    "    if use_projection or shortcut.shape[-1] != filters or stride != 1:\n",
    "        shortcut = layers.Conv2D(filters, (1, 1), strides=stride, padding=\"same\",\n",
    "                                 use_bias=False, name=None if name is None else name + \"_proj_conv\")(shortcut)\n",
    "        shortcut = layers.BatchNormalization(axis=-1, name=None if name is None else name + \"_proj_bn\")(shortcut)\n",
    "\n",
    "    # Add skip connection and apply final ReLU\n",
    "    x = layers.Add(name=None if name is None else name + \"_add\")([x, shortcut])\n",
    "    x = layers.ReLU(name=None if name is None else name + \"_out\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_resnet18(input_shape=(64, 64, 3), num_classes=200):\n",
    "    \"\"\"\n",
    "    ResNet18 style model for Tiny ImageNet (64x64x3 inputs, 200 classes).\n",
    "    Based on the standard 2-2-2-2 block layout.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Initial conv: 3x3 instead of 7x7 for Tiny ImageNet resolution\n",
    "    x = layers.Conv2D(64, (3, 3), strides=1, padding=\"same\",\n",
    "                      use_bias=False, name=\"conv1\")(inputs)\n",
    "    x = layers.BatchNormalization(axis=-1, name=\"bn1\")(x)\n",
    "    x = layers.ReLU(name=\"relu1\")(x)\n",
    "\n",
    "    # Stage 1: 2 blocks, 64 filters\n",
    "    x = residual_block(x, 64, stride=1, use_projection=False, name=\"stage1_block1\")\n",
    "    x = residual_block(x, 64, stride=1, use_projection=False, name=\"stage1_block2\")\n",
    "\n",
    "    # Stage 2: 2 blocks, 128 filters, first block downsamples\n",
    "    x = residual_block(x, 128, stride=2, use_projection=True, name=\"stage2_block1\")\n",
    "    x = residual_block(x, 128, stride=1, use_projection=False, name=\"stage2_block2\")\n",
    "\n",
    "    # Stage 3: 2 blocks, 256 filters, first block downsamples\n",
    "    x = residual_block(x, 256, stride=2, use_projection=True, name=\"stage3_block1\")\n",
    "    x = residual_block(x, 256, stride=1, use_projection=False, name=\"stage3_block2\")\n",
    "\n",
    "    # Stage 4: 2 blocks, 512 filters, first block downsamples\n",
    "    x = residual_block(x, 512, stride=2, use_projection=True, name=\"stage4_block1\")\n",
    "    x = residual_block(x, 512, stride=1, use_projection=False, name=\"stage4_block2\")\n",
    "\n",
    "    # Global average pooling and classifier\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"fc\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=\"ResNet18_TinyImageNet\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build and compile the model\n",
    "resnet_model = build_resnet18(input_shape=(64, 64, 3), num_classes=200)\n",
    "\n",
    "resnet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "resnet_model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388f9e6",
   "metadata": {},
   "source": [
    "## Model Compilation and Training\n",
    "\n",
    "This cell compiles and trains the CNN model.\n",
    "\n",
    "### Compilation\n",
    "The Adam optimizer is used because it adapts the learning rate during training and performs well across a wide range of deep learning tasks.  \n",
    "The loss function is categorical crossentropy, which is standard for multi class classification problems.  \n",
    "Accuracy is used as the evaluation metric.\n",
    "\n",
    "### Training Strategy\n",
    "The model is trained for a fixed number of epochs using the training generator.  \n",
    "The validation generator is used to measure generalization performance across epochs.  \n",
    "This allows us to observe potential overfitting and adjust the model or regularization if needed.\n",
    "\n",
    "The training history is stored so that accuracy and loss curves can be plotted in the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate generators to make sure they are in sync with the current graph\n",
    "IMAGE_SIZE = (64, 64)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(DATA_ROOT, \"train\"),\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    os.path.join(DATA_ROOT, \"val\"),\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Compile with a solid baseline learning rate\n",
    "resnet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "EPOCHS = 15  # you can lower this to 3 just to verify training\n",
    "history = resnet_model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8e450",
   "metadata": {},
   "source": [
    "## Training Curves\n",
    "\n",
    "This cell plots the training and validation accuracy and loss across epochs.  \n",
    "These plots help evaluate whether the model is learning effectively and whether overfitting is occurring.  \n",
    "A healthy model typically shows increasing accuracy and decreasing loss on both training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07fa281",
   "metadata": {},
   "source": [
    "## Classification Report (Precision, Recall, and F1 Score)\n",
    "\n",
    "Accuracy alone does not fully describe model performance for a 200 class dataset.  \n",
    "Precision, recall, and F1 score provide a more detailed view of how the model behaves on each class.\n",
    "\n",
    "Macro averaged precision treats each class equally and measures how often predictions for that class are correct.  \n",
    "Macro averaged recall measures how often the model correctly identifies images from each class.  \n",
    "Macro averaged F1 combines precision and recall into a single balanced score.\n",
    "\n",
    "This report is generated using scikit learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9378b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the validation set\n",
    "val_generator.reset()\n",
    "pred_probs = model.predict(val_generator)\n",
    "pred_classes = np.argmax(pred_probs, axis=1)\n",
    "true_classes = val_generator.classes\n",
    "\n",
    "# Produce classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class_labels = list(val_generator.class_indices.keys())\n",
    "report = classification_report(true_classes, pred_classes, target_names=class_labels, zero_division=0)\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0138146",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The confusion matrix shows how the model performs on each class.  \n",
    "Diagonal entries correspond to correct predictions.  \n",
    "Off diagonal entries show misclassifications.\n",
    "\n",
    "Visualizing the confusion matrix helps reveal which classes are frequently confused.  \n",
    "For a 200 class dataset, the full matrix is large, so the heatmap focuses on general patterns rather than individual labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fdbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(true_classes, pred_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, cmap=\"Blues\", cbar=True)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834772e",
   "metadata": {},
   "source": [
    "## Top 5 Accuracy\n",
    "\n",
    "Top 5 accuracy measures whether the correct class appears among the five most probable predictions.  \n",
    "This metric is widely used for large multiclass datasets such as ImageNet and Tiny ImageNet because a single mistake among 200 classes does not always indicate meaningful failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fa55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute top 5 accuracy manually\n",
    "top5_correct = 0\n",
    "top5_total = len(true_classes)\n",
    "\n",
    "top5_preds = np.argsort(pred_probs, axis=1)[:, -5:]  # last 5 entries are the highest\n",
    "\n",
    "for i in range(top5_total):\n",
    "    if true_classes[i] in top5_preds[i]:\n",
    "        top5_correct += 1\n",
    "\n",
    "top5_accuracy = top5_correct / top5_total\n",
    "print(\"Top 5 Accuracy:\", top5_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fcd3a7",
   "metadata": {},
   "source": [
    "## Evaluation Summary\n",
    "\n",
    "This notebook reports several metrics to evaluate model performance on Tiny ImageNet.\n",
    "\n",
    "1. Accuracy  \n",
    "Measures overall correctness of the predictions.\n",
    "\n",
    "2. Loss  \n",
    "Measures prediction confidence and fit to the data.\n",
    "\n",
    "3. Precision, Recall, and F1 Score  \n",
    "Provide per class evaluation and reveal strengths and weaknesses beyond accuracy.\n",
    "\n",
    "4. Confusion Matrix  \n",
    "Shows patterns of misclassification between classes.\n",
    "\n",
    "5. Top 5 Accuracy  \n",
    "Indicates whether the true class appears among the five most likely predictions and is a standard metric for large multiclass datasets.\n",
    "\n",
    "These metrics together provide a complete picture of model behavior and generalization ability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288236dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny215",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
