{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d67f365",
      "metadata": {
        "id": "3d67f365"
      },
      "source": [
        "### Environment and Device Check  \n",
        "Before training the model, it is important to verify that TensorFlow is installed correctly and to identify which devices are available for computation.  \n",
        "This cell prints the TensorFlow version, lists all logical devices detected by TensorFlow, and reports the number of GPUs available.  \n",
        "This determines whether training will run on CPU or GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d269c0f0",
      "metadata": {
        "id": "d269c0f0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Available devices:\")\n",
        "for d in tf.config.list_logical_devices():\n",
        "    print(\" \", d)\n",
        "\n",
        "print(\"Num GPUs:\", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f95e085",
      "metadata": {
        "id": "0f95e085"
      },
      "source": [
        "### Install kagglehub  \n",
        "This project uses Tiny-ImageNet downloaded through **kagglehub**, so we need to install the package before accessing the dataset.  \n",
        "This cell installs kagglehub inside the current environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b19fda9b",
      "metadata": {
        "id": "b19fda9b"
      },
      "outputs": [],
      "source": [
        "!pip install kagglehub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e1e339",
      "metadata": {
        "id": "a9e1e339"
      },
      "source": [
        "### Download Tiny-ImageNet  \n",
        "We use `kagglehub` to download the Tiny-ImageNet dataset.  \n",
        "This block retrieves the dataset, prints the local path, and sets the project root directory so later cells can load images correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4bf81cff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bf81cff",
        "outputId": "38bbe326-e222-4ac3-ecac-155e0714edb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'tiny-imagenet' dataset.\n",
            "KaggleHub source: /kaggle/input/tiny-imagenet\n",
            "Top-level contents: ['tiny-imagenet-200']\n",
            "ROOT = /kaggle/input/tiny-imagenet/tiny-imagenet-200\n",
            "Root contents: ['words.txt', 'wnids.txt', 'tiny-imagenet-200', 'val', 'test', 'train']\n"
          ]
        }
      ],
      "source": [
        "import kagglehub, os\n",
        "\n",
        "# Download using KaggleHub (cached on Colab)\n",
        "src = kagglehub.dataset_download(\"akash2sharma/tiny-imagenet\")\n",
        "\n",
        "print(\"KaggleHub source:\", src)\n",
        "print(\"Top-level contents:\", os.listdir(src))\n",
        "\n",
        "# This is the correct dataset root\n",
        "root = os.path.join(src, \"tiny-imagenet-200\")\n",
        "print(\"ROOT =\", root)\n",
        "\n",
        "# Show contents\n",
        "print(\"Root contents:\", os.listdir(root))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "print(\"=== Train class count ===\")\n",
        "out = subprocess.check_output(\n",
        "    f\"find /kaggle/input/tiny-imagenet/tiny-imagenet-200/train -maxdepth 1 -type d | wc -l\",\n",
        "    shell=True\n",
        ")\n",
        "print(out.decode())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr2XNatMIMwv",
        "outputId": "7354c6ed-9120-48f1-c28b-d7c639f7193a"
      },
      "id": "Jr2XNatMIMwv",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Train class count ===\n",
            "201\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Train directory sample ===\")\n",
        "out = subprocess.check_output(\n",
        "    \"ls -lha /kaggle/input/tiny-imagenet/tiny-imagenet-200/train | head\",\n",
        "    shell=True\n",
        ")\n",
        "print(out.decode())\n"
      ],
      "metadata": {
        "id": "39pHEvFQOWd-",
        "outputId": "c21a1d91-8f1d-4149-a6da-789a388ddd2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "39pHEvFQOWd-",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Train directory sample ===\n",
            "total 0\n",
            "drwxr-sr-x 202 1000 1000 0 Nov 24 22:17 .\n",
            "drwxr-sr-x   6 1000 1000 0 Nov 24 22:18 ..\n",
            "drwxr-sr-x   3 1000 1000 0 Nov 24 22:15 n01443537\n",
            "drwxr-sr-x   3 1000 1000 0 Nov 24 22:15 n01629819\n",
            "drwxr-sr-x   3 1000 1000 0 Nov 24 22:15 n01641577\n",
            "drwxr-sr-x   3 1000 1000 0 Nov 24 22:15 n01644900\n",
            "drwxr-sr-x   3 1000 1000 0 Nov 24 22:15 n01698640\n",
            "drwxr-sr-x   3 1000 1000 0 Nov 24 22:16 n01742172\n",
            "drwxr-sr-x   3 1000 1000 0 Nov 24 22:16 n01768244\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Val folder structure ===\")\n",
        "out = subprocess.check_output(\n",
        "    \"ls -lha /kaggle/input/tiny-imagenet/tiny-imagenet-200/val\",\n",
        "    shell=True\n",
        ")\n",
        "print(out.decode())\n"
      ],
      "metadata": {
        "id": "SjP9LS4_OfBm",
        "outputId": "310a4619-e1ed-45a8-bea8-f36f15deecde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SjP9LS4_OfBm",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Val folder structure ===\n",
            "total 340K\n",
            "drwxr-sr-x 3 1000 1000    0 Nov 24 22:18 .\n",
            "drwxr-sr-x 6 1000 1000    0 Nov 24 22:18 ..\n",
            "drwxr-sr-x 2 1000 1000    0 Nov 24 22:18 images\n",
            "-rw-r--r-- 1 1000 1000 340K Nov 24 22:18 val_annotations.txt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67c9d257",
      "metadata": {
        "id": "67c9d257"
      },
      "source": [
        "### Load training images normally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "17ecebd7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17ecebd7",
        "outputId": "31692bbb-fe72-484b-f5e9-8ee145b16960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Classes in training: 200\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMG_SIZE = 64\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_gen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "train_generator = train_gen.flow_from_directory(\n",
        "    os.path.join(root, \"train\"),\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "num_classes = train_generator.num_classes\n",
        "print(\"Classes in training:\", num_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Validation DataFrame"
      ],
      "metadata": {
        "id": "KamBKLoBQFVG"
      },
      "id": "KamBKLoBQFVG"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "val_dir = os.path.join(root, \"val\")\n",
        "val_images = os.path.join(val_dir, \"images\")\n",
        "annotations_file = os.path.join(val_dir, \"val_annotations.txt\")\n",
        "\n",
        "df_val = pd.read_csv(annotations_file, sep=\"\\t\", header=None)\n",
        "df_val.columns = [\"filename\", \"class\", \"x1\", \"y1\", \"x2\", \"y2\"]\n",
        "\n",
        "df_val[\"filepath\"] = df_val[\"filename\"].apply(lambda f: os.path.join(val_images, f))\n",
        "\n",
        "print(df_val.head())\n",
        "print(\"Val samples:\", len(df_val))\n"
      ],
      "metadata": {
        "id": "6iW5D8dCQEN0",
        "outputId": "497ceba0-5d83-48d3-8688-52a1eb8974c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6iW5D8dCQEN0",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     filename      class  x1  y1  x2  y2  \\\n",
            "0  val_0.JPEG  n03444034   0  32  44  62   \n",
            "1  val_1.JPEG  n04067472  52  55  57  59   \n",
            "2  val_2.JPEG  n04070727   4   0  60  55   \n",
            "3  val_3.JPEG  n02808440   3   3  63  63   \n",
            "4  val_4.JPEG  n02808440   9  27  63  48   \n",
            "\n",
            "                                            filepath  \n",
            "0  /kaggle/input/tiny-imagenet/tiny-imagenet-200/...  \n",
            "1  /kaggle/input/tiny-imagenet/tiny-imagenet-200/...  \n",
            "2  /kaggle/input/tiny-imagenet/tiny-imagenet-200/...  \n",
            "3  /kaggle/input/tiny-imagenet/tiny-imagenet-200/...  \n",
            "4  /kaggle/input/tiny-imagenet/tiny-imagenet-200/...  \n",
            "Val samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb74473",
      "metadata": {
        "id": "1eb74473"
      },
      "source": [
        "### Validation Set Generator\n",
        "\n",
        "Building a custom generator that loads images directly from file paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fcd0e80f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcd0e80f",
        "outputId": "2fedd3b1-581d-4eb3-bb43-53daa2635bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation generator ready.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "class TinyImageNetValGenerator(Sequence):\n",
        "    def __init__(self, df, class_indices, batch_size=64, img_size=64):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.class_indices = class_indices\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.indices = np.arange(len(df))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_idx = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "        batch = self.df.iloc[batch_idx]\n",
        "\n",
        "        X = np.zeros((len(batch), self.img_size, self.img_size, 3), dtype=\"float32\")\n",
        "        y = np.zeros((len(batch), len(self.class_indices)), dtype=\"float32\")\n",
        "\n",
        "        for i, (_, row) in enumerate(batch.iterrows()):\n",
        "            img = load_img(row.filepath, target_size=(self.img_size, self.img_size))\n",
        "            arr = img_to_array(img) / 255.0\n",
        "            X[i] = arr\n",
        "            y[i, self.class_indices[row[\"class\"]]] = 1.0\n",
        "\n",
        "        return X, y\n",
        "\n",
        "val_generator = TinyImageNetValGenerator(\n",
        "    df_val,\n",
        "    class_indices=train_generator.class_indices,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    img_size=IMG_SIZE\n",
        ")\n",
        "\n",
        "print(\"Validation generator ready.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3ed0d3",
      "metadata": {
        "id": "1d3ed0d3"
      },
      "source": [
        "Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a1d90134",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1d90134",
        "outputId": "3cb55e6e-e425-4626-b3b4-26a477518492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train X: (64, 64, 64, 3)\n",
            "Train Y: (64, 200)\n",
            "Val X: (64, 64, 64, 3)\n",
            "Val Y: (64, 200)\n",
            "Train num classes: 200\n",
            "Val num classes: 200\n"
          ]
        }
      ],
      "source": [
        "# Check training batch\n",
        "x_train, y_train = next(train_generator)\n",
        "print(\"Train X:\", x_train.shape)\n",
        "print(\"Train Y:\", y_train.shape)\n",
        "\n",
        "# Check validation batch\n",
        "x_val, y_val = val_generator[0]\n",
        "print(\"Val X:\", x_val.shape)\n",
        "print(\"Val Y:\", y_val.shape)\n",
        "\n",
        "print(\"Train num classes:\", y_train.shape[1])\n",
        "print(\"Val num classes:\", y_val.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6acd309d",
      "metadata": {
        "id": "6acd309d"
      },
      "source": [
        "### Define the Custom ResNet-18 Model  \n",
        "This cell implements a lightweight ResNet-18 suitable for Tiny-ImageNet.  \n",
        "It includes:  \n",
        "- A residual block with optional projection  \n",
        "- Downsampling at each stage  \n",
        "- Global average pooling  \n",
        "- Final softmax classification head  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4397da5a",
      "metadata": {
        "id": "4397da5a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def residual_block(x, filters, stride=1, use_projection=False, name=None):\n",
        "    shortcut = x\n",
        "\n",
        "    x = layers.Conv2D(filters, 3, stride, padding=\"same\", use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    x = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if use_projection:\n",
        "        shortcut = layers.Conv2D(filters, 1, stride, padding=\"same\", use_bias=False)(shortcut)\n",
        "        shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    x = layers.Add()([x, shortcut])\n",
        "    x = layers.ReLU()(x)\n",
        "    return x\n",
        "\n",
        "def build_resnet18(input_shape=(64, 64, 3), num_classes=200):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv2D(64, 3, padding=\"same\", use_bias=False)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    x = residual_block(x, 64, stride=1, use_projection=False)\n",
        "    x = residual_block(x, 64, stride=1, use_projection=False)\n",
        "\n",
        "    x = residual_block(x, 128, stride=2, use_projection=True)\n",
        "    x = residual_block(x, 128, stride=1, use_projection=False)\n",
        "\n",
        "    x = residual_block(x, 256, stride=2, use_projection=True)\n",
        "    x = residual_block(x, 256, stride=1, use_projection=False)\n",
        "\n",
        "    x = residual_block(x, 512, stride=2, use_projection=True)\n",
        "    x = residual_block(x, 512, stride=1, use_projection=False)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return models.Model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34eb4a8",
      "metadata": {
        "id": "a34eb4a8"
      },
      "source": [
        "### Compile the ResNet-18 Model  \n",
        "We compile using Adam with a stable learning rate.  \n",
        "Loss is categorical cross entropy because Tiny-ImageNet labels are one hot.  \n",
        "Accuracy is tracked along with planned later metrics (Top 5 accuracy).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f71626d4",
      "metadata": {
        "id": "f71626d4"
      },
      "outputs": [],
      "source": [
        "model = build_resnet18()\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\n",
        "        \"accuracy\",\n",
        "        tf.keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top5_accuracy\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c21c3b",
      "metadata": {
        "id": "35c21c3b"
      },
      "source": [
        "### Add Training Callbacks  \n",
        "We include two callbacks to stabilize training:\n",
        "\n",
        "1. **EarlyStopping**  \n",
        "   Stops training when validation accuracy stops improving.  \n",
        "   This prevents overfitting and wasted compute.\n",
        "\n",
        "2. **ModelCheckpoint**  \n",
        "   Saves the best performing model during training.  \n",
        "   We will reload this model later for final evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f7771d",
      "metadata": {
        "id": "70f7771d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "checkpoint_path = \"best_resnet18.h5\"\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor=\"val_accuracy\",\n",
        "        patience=3,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7831b7ee",
      "metadata": {
        "id": "7831b7ee"
      },
      "source": [
        "### Train the ResNet-18 Model  \n",
        "We now begin full training using the GPU.  \n",
        "The model will run for up to 20 epochs, but **EarlyStopping** may stop it earlier if validation accuracy plateaus.\n",
        "\n",
        "Training includes:\n",
        "- Feedforward and backprop on the training set  \n",
        "- Validation tracking every epoch  \n",
        "- Checkpoint saving for the best model  \n",
        "- Early stopping to avoid overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e47aa9cd",
      "metadata": {
        "id": "e47aa9cd"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20  # GPU-friendly\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e09229d1",
      "metadata": {
        "id": "e09229d1"
      },
      "source": [
        "### Plot Training Curves  \n",
        "To understand learning behavior, we plot both accuracy and loss for training and validation.  \n",
        "This helps visualize:\n",
        "- Whether the model is overfitting  \n",
        "- Whether accuracy is improving across epochs  \n",
        "- If loss is decreasing or plateauing  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b06a6b9",
      "metadata": {
        "id": "6b06a6b9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history[\"accuracy\"], label=\"train\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"val\")\n",
        "plt.title(\"Accuracy\"); plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history[\"loss\"], label=\"train\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
        "plt.title(\"Loss\"); plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02533b95",
      "metadata": {
        "id": "02533b95"
      },
      "source": [
        "### Collect Model Predictions for Confusion Matrix  \n",
        "We gather predictions for the entire validation set by iterating over the validation generator.  \n",
        "Steps:  \n",
        "1. Reverse the `class_indices` mapping to convert integer predictions back to class names.  \n",
        "2. Loop through all validation batches.  \n",
        "3. Collect ground truth labels (`y_true`) and predicted labels (`y_pred`).  \n",
        "4. Convert them to numpy arrays for confusion matrix computation.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3df9244",
      "metadata": {
        "id": "e3df9244"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get class order\n",
        "class_indices = train_generator.class_indices\n",
        "idx_to_class = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "# Predict all validation samples\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "val_generator.reset()\n",
        "\n",
        "for _ in range(len(val_generator)):\n",
        "    batch_x, batch_y = next(val_generator)\n",
        "    preds = model.predict(batch_x, verbose=0)\n",
        "    y_true.extend(np.argmax(batch_y, axis=1))\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "print(\"Prediction collection complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c58cee57",
      "metadata": {
        "id": "c58cee57"
      },
      "source": [
        "### Confusion Matrix Visualization and Export  \n",
        "To evaluate how well the model distinguishes between different categories, we compute the confusion matrix across all 200 classes.  \n",
        "Since plotting all 200 categories at once becomes unreadable, we visualize only the first 20 classes as a heatmap.  \n",
        "\n",
        "Steps:  \n",
        "1. Compute the full confusion matrix using all validation predictions.  \n",
        "2. Extract a smaller 20x20 subset for readable visualization.  \n",
        "3. Plot a heatmap with seaborn.  \n",
        "4. Save the full confusion matrix as a CSV file for further analysis or reporting.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62586a56",
      "metadata": {
        "id": "62586a56"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion matrix shape:\", cm.shape)\n",
        "\n",
        "subset_classes = list(range(20))\n",
        "\n",
        "cm_subset = cm[np.ix_(subset_classes, subset_classes)]\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(cm_subset, annot=False, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix (first 20 classes)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "df_cm = pd.DataFrame(cm)\n",
        "df_cm.to_csv(\"tiny_imagenet_confusion_matrix.csv\", index=False)\n",
        "print(\"Confusion matrix saved to tiny_imagenet_confusion_matrix.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3284120d",
      "metadata": {
        "id": "3284120d"
      },
      "source": [
        "### Evaluating the Best Saved Model  \n",
        "After training with early stopping and checkpointing, the model with the highest validation accuracy is loaded from disk.  \n",
        "To measure performance, we compute:  \n",
        "\n",
        "- Final loss on the validation set  \n",
        "- Top 1 accuracy  \n",
        "- Top 5 accuracy  \n",
        "\n",
        "Top 1 accuracy indicates whether the highest softmax probability matches the true class.  \n",
        "Top 5 accuracy checks whether the correct label appears within the five most likely predictions.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43edd994",
      "metadata": {
        "id": "43edd994"
      },
      "outputs": [],
      "source": [
        "best_model = tf.keras.models.load_model(\"best_resnet18.h5\")\n",
        "\n",
        "results = best_model.evaluate(val_generator)\n",
        "print(\"Best model results:\", results)\n",
        "print(\"Top 1 accuracy:\", results[1])\n",
        "print(\"Top 5 accuracy:\", results[2])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tiny215",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}