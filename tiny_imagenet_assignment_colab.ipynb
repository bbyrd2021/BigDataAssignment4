{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d67f365",
   "metadata": {},
   "source": [
    "### Environment and Device Check  \n",
    "Before training the model, it is important to verify that TensorFlow is installed correctly and to identify which devices are available for computation.  \n",
    "This cell prints the TensorFlow version, lists all logical devices detected by TensorFlow, and reports the number of GPUs available.  \n",
    "This determines whether training will run on CPU or GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d269c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Available devices:\")\n",
    "for d in tf.config.list_logical_devices():\n",
    "    print(\" \", d)\n",
    "\n",
    "print(\"Num GPUs:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f95e085",
   "metadata": {},
   "source": [
    "### Install kagglehub  \n",
    "This project uses Tiny-ImageNet downloaded through **kagglehub**, so we need to install the package before accessing the dataset.  \n",
    "This cell installs kagglehub inside the current environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19fda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglehub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1e339",
   "metadata": {},
   "source": [
    "### Download Tiny-ImageNet  \n",
    "We use `kagglehub` to download the Tiny-ImageNet dataset.  \n",
    "This block retrieves the dataset, prints the local path, and sets the project root directory so later cells can load images correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf81cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Download kaggle dataset\n",
    "src = kagglehub.dataset_download(\"akash2sharma/tiny-imagenet\")\n",
    "\n",
    "# Copy only once into /content\n",
    "root = \"/content/tiny-imagenet-200\"\n",
    "\n",
    "if not os.path.exists(root):\n",
    "    shutil.copytree(os.path.join(src, \"tiny-imagenet-200\"), root)\n",
    "\n",
    "print(\"Dataset ready at:\", root)\n",
    "print(\"Contents:\", os.listdir(root))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a53507",
   "metadata": {},
   "source": [
    "### Reorganize Validation Set  \n",
    "Load val annotations without moving images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb49ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "val_annotations = os.path.join(root, \"val\", \"val_annotations.txt\")\n",
    "\n",
    "df_val = pd.read_csv(val_annotations, sep=\"\\t\", header=None)\n",
    "df_val.columns = [\"filename\", \"class\", \"x1\", \"y1\", \"x2\", \"y2\"]\n",
    "\n",
    "# Add full path to images\n",
    "df_val[\"filepath\"] = df_val[\"filename\"].apply(\n",
    "    lambda f: os.path.join(root, \"val\", \"images\", f)\n",
    ")\n",
    "\n",
    "print(df_val.head())\n",
    "print(\"Validation samples:\", len(df_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c9d257",
   "metadata": {},
   "source": [
    "### Load training images normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_gen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_generator = train_gen.flow_from_directory(\n",
    "    os.path.join(root, \"train\"),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "print(\"Classes:\", num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb74473",
   "metadata": {},
   "source": [
    "### Validation Set Generator\n",
    "\n",
    "Building a custom generator that loads images directly from file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# Map class names to indices (same as training)\n",
    "class_to_idx = train_generator.class_indices\n",
    "\n",
    "# Keep only rows where class is valid\n",
    "df_val = df_val[df_val[\"class\"].isin(class_to_idx.keys())].reset_index(drop=True)\n",
    "print(\"Validation filtered:\", len(df_val))\n",
    "\n",
    "\n",
    "class ValSequence(Sequence):\n",
    "    def __init__(self, df, batch_size, img_size):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_df = self.df.iloc[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for _, row in batch_df.iterrows():\n",
    "            img = load_img(row[\"filepath\"], target_size=(self.img_size, self.img_size))\n",
    "            img = img_to_array(img) / 255.0\n",
    "            images.append(img)\n",
    "\n",
    "            label_vec = np.zeros(len(self.class_to_idx))\n",
    "            label_vec[self.class_to_idx[row[\"class\"]]] = 1\n",
    "            labels.append(label_vec)\n",
    "\n",
    "        return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "val_generator = ValSequence(df_val, batch_size=BATCH_SIZE, img_size=IMG_SIZE)\n",
    "\n",
    "print(\"Validation batches:\", len(val_generator))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ed0d3",
   "metadata": {},
   "source": [
    "Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d90134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training batch\n",
    "x_train, y_train = next(train_generator)\n",
    "print(\"Train X:\", x_train.shape)\n",
    "print(\"Train Y:\", y_train.shape)\n",
    "\n",
    "# Check validation batch\n",
    "x_val, y_val = val_generator[0]\n",
    "print(\"Val X:\", x_val.shape)\n",
    "print(\"Val Y:\", y_val.shape)\n",
    "\n",
    "print(\"Train num classes:\", y_train.shape[1])\n",
    "print(\"Val num classes:\", y_val.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acd309d",
   "metadata": {},
   "source": [
    "### Define the Custom ResNet-18 Model  \n",
    "This cell implements a lightweight ResNet-18 suitable for Tiny-ImageNet.  \n",
    "It includes:  \n",
    "- A residual block with optional projection  \n",
    "- Downsampling at each stage  \n",
    "- Global average pooling  \n",
    "- Final softmax classification head  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4397da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def residual_block(x, filters, stride=1, use_projection=False, name=None):\n",
    "    shortcut = x\n",
    "\n",
    "    x = layers.Conv2D(filters, 3, stride, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if use_projection:\n",
    "        shortcut = layers.Conv2D(filters, 1, stride, padding=\"same\", use_bias=False)(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def build_resnet18(input_shape=(64, 64, 3), num_classes=200):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = residual_block(x, 64, stride=1, use_projection=False)\n",
    "    x = residual_block(x, 64, stride=1, use_projection=False)\n",
    "\n",
    "    x = residual_block(x, 128, stride=2, use_projection=True)\n",
    "    x = residual_block(x, 128, stride=1, use_projection=False)\n",
    "\n",
    "    x = residual_block(x, 256, stride=2, use_projection=True)\n",
    "    x = residual_block(x, 256, stride=1, use_projection=False)\n",
    "\n",
    "    x = residual_block(x, 512, stride=2, use_projection=True)\n",
    "    x = residual_block(x, 512, stride=1, use_projection=False)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34eb4a8",
   "metadata": {},
   "source": [
    "### Compile the ResNet-18 Model  \n",
    "We compile using Adam with a stable learning rate.  \n",
    "Loss is categorical cross entropy because Tiny-ImageNet labels are one hot.  \n",
    "Accuracy is tracked along with planned later metrics (Top 5 accuracy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71626d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_resnet18()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top5_accuracy\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c21c3b",
   "metadata": {},
   "source": [
    "### Add Training Callbacks  \n",
    "We include two callbacks to stabilize training:\n",
    "\n",
    "1. **EarlyStopping**  \n",
    "   Stops training when validation accuracy stops improving.  \n",
    "   This prevents overfitting and wasted compute.\n",
    "\n",
    "2. **ModelCheckpoint**  \n",
    "   Saves the best performing model during training.  \n",
    "   We will reload this model later for final evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f7771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "checkpoint_path = \"best_resnet18.h5\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831b7ee",
   "metadata": {},
   "source": [
    "### Train the ResNet-18 Model  \n",
    "We now begin full training using the GPU.  \n",
    "The model will run for up to 20 epochs, but **EarlyStopping** may stop it earlier if validation accuracy plateaus.\n",
    "\n",
    "Training includes:\n",
    "- Feedforward and backprop on the training set  \n",
    "- Validation tracking every epoch  \n",
    "- Checkpoint saving for the best model  \n",
    "- Early stopping to avoid overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47aa9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20  # GPU-friendly\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09229d1",
   "metadata": {},
   "source": [
    "### Plot Training Curves  \n",
    "To understand learning behavior, we plot both accuracy and loss for training and validation.  \n",
    "This helps visualize:\n",
    "- Whether the model is overfitting  \n",
    "- Whether accuracy is improving across epochs  \n",
    "- If loss is decreasing or plateauing  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val\")\n",
    "plt.title(\"Accuracy\"); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
    "plt.title(\"Loss\"); plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02533b95",
   "metadata": {},
   "source": [
    "### Collect Model Predictions for Confusion Matrix  \n",
    "We gather predictions for the entire validation set by iterating over the validation generator.  \n",
    "Steps:  \n",
    "1. Reverse the `class_indices` mapping to convert integer predictions back to class names.  \n",
    "2. Loop through all validation batches.  \n",
    "3. Collect ground truth labels (`y_true`) and predicted labels (`y_pred`).  \n",
    "4. Convert them to numpy arrays for confusion matrix computation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get class order\n",
    "class_indices = train_generator.class_indices\n",
    "idx_to_class = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "# Predict all validation samples\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "val_generator.reset()\n",
    "\n",
    "for _ in range(len(val_generator)):\n",
    "    batch_x, batch_y = next(val_generator)\n",
    "    preds = model.predict(batch_x, verbose=0)\n",
    "    y_true.extend(np.argmax(batch_y, axis=1))\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print(\"Prediction collection complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58cee57",
   "metadata": {},
   "source": [
    "### Confusion Matrix Visualization and Export  \n",
    "To evaluate how well the model distinguishes between different categories, we compute the confusion matrix across all 200 classes.  \n",
    "Since plotting all 200 categories at once becomes unreadable, we visualize only the first 20 classes as a heatmap.  \n",
    "\n",
    "Steps:  \n",
    "1. Compute the full confusion matrix using all validation predictions.  \n",
    "2. Extract a smaller 20x20 subset for readable visualization.  \n",
    "3. Plot a heatmap with seaborn.  \n",
    "4. Save the full confusion matrix as a CSV file for further analysis or reporting.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62586a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion matrix shape:\", cm.shape)\n",
    "\n",
    "subset_classes = list(range(20))\n",
    "\n",
    "cm_subset = cm[np.ix_(subset_classes, subset_classes)]\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cm_subset, annot=False, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix (first 20 classes)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "df_cm = pd.DataFrame(cm)\n",
    "df_cm.to_csv(\"tiny_imagenet_confusion_matrix.csv\", index=False)\n",
    "print(\"Confusion matrix saved to tiny_imagenet_confusion_matrix.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284120d",
   "metadata": {},
   "source": [
    "### Evaluating the Best Saved Model  \n",
    "After training with early stopping and checkpointing, the model with the highest validation accuracy is loaded from disk.  \n",
    "To measure performance, we compute:  \n",
    "\n",
    "- Final loss on the validation set  \n",
    "- Top 1 accuracy  \n",
    "- Top 5 accuracy  \n",
    "\n",
    "Top 1 accuracy indicates whether the highest softmax probability matches the true class.  \n",
    "Top 5 accuracy checks whether the correct label appears within the five most likely predictions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43edd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(\"best_resnet18.h5\")\n",
    "\n",
    "results = best_model.evaluate(val_generator)\n",
    "print(\"Best model results:\", results)\n",
    "print(\"Top 1 accuracy:\", results[1])\n",
    "print(\"Top 5 accuracy:\", results[2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny215",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
